{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa7bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "\n",
    "path = r'F:\\Dataset_open_topic\\maestro-v3.0.0_all\\maestro-v3.0.0'\n",
    "file = \"maestro-v3.0.0.csv\"\n",
    "file_name = os.path.join(path, file)\n",
    "df = pd.read_csv(file_name)  # or your cleaned version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155f4875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['split'] == 'train']\n",
    "val_df   = df[df['split'] == 'validation']\n",
    "test_df  = df[df['split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b721ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = list(zip(train_df['audio_filename'], train_df['midi_filename']))\n",
    "val_paths   = list(zip(val_df['audio_filename'], val_df['midi_filename']))\n",
    "test_paths  = list(zip(test_df['audio_filename'], test_df['midi_filename']))\n",
    "\n",
    "# Example: get full path\n",
    "full_audio_path = os.path.join(path, train_paths[0][0])\n",
    "full_midi_path = os.path.join(path, train_paths[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8deb53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "SR = 22050\n",
    "HOP_LENGTH_SEC = 0.01\n",
    "HOP_LENGTH = int(HOP_LENGTH_SEC * SR)\n",
    "FMIN = 27.5\n",
    "BINS_PER_OCTAVE = 36\n",
    "N_BINS = 267\n",
    "NUM_CLASSES = 128\n",
    "WINDOW_SIZE = 9\n",
    "STRIDE = 1\n",
    "\n",
    "# Training \n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf55239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_audio_and_midi(audio_path, midi_path):\n",
    "    # Compute CQT\n",
    "    y, _ = librosa.load(audio_path, sr=SR)\n",
    "    C = librosa.cqt(\n",
    "        y, sr=SR, hop_length=HOP_LENGTH, fmin=FMIN,\n",
    "        n_bins=N_BINS, bins_per_octave=BINS_PER_OCTAVE\n",
    "    )\n",
    "    C_dB = librosa.amplitude_to_db(np.abs(C), ref=np.max)\n",
    "\n",
    "    # Compute Piano Roll\n",
    "    midi = pretty_midi.PrettyMIDI(midi_path)\n",
    "    piano_roll = midi.get_piano_roll(fs=SR / HOP_LENGTH)\n",
    "\n",
    "    # Align length\n",
    "    n_frames = min(C_dB.shape[1], piano_roll.shape[1])\n",
    "    return C_dB[:, :n_frames], piano_roll[:, :n_frames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6fe0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "cqt_list, piano_list = [], []\n",
    "for audio_rel, midi_rel in tqdm(train_paths[:10]):\n",
    "    cqt, piano = load_audio_and_midi(os.path.join(path, audio_rel),\n",
    "                                     os.path.join(path, midi_rel))\n",
    "    cqt_list.append(cqt)\n",
    "    piano_list.append(piano)\n",
    "\n",
    "cqt_train = np.concatenate(cqt_list, axis=1)\n",
    "piano_train = np.concatenate(piano_list, axis=1)\n",
    "np.savez_compressed('train_data.npz', cqt=cqt_train, piano=piano_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb482a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('train_data.npz')\n",
    "\n",
    "cqt_train = data['cqt']\n",
    "piano_train = data['piano']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691be17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# path_train = r'F:\\Dataset_open_topic\\train_individual'\n",
    "# cqt_train = []\n",
    "# piano_train = []\n",
    "# i = 0\n",
    "# for file in tqdm(os.listdir(path_train)):\n",
    "#     # print(file)\n",
    "#     if i < 10:\n",
    "#         file = os.path.join(path_train, file)\n",
    "#         data = np.load(file)\n",
    "#         cqt_train.append(data['cqt'])\n",
    "#         piano_train.append(data['piano'])\n",
    "#     i += 1\n",
    "\n",
    "# cqt_train = np.concatenate(cqt_train, axis=1)\n",
    "# piano_train = np.concatenate(piano_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da00e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(data, window_size=9, stride=1, pad_mode='edge', constant_value=0):\n",
    "    pad = window_size // 2\n",
    "    pad_width = ((0, 0), (pad, pad))\n",
    "\n",
    "    if pad_mode == 'constant':\n",
    "        padded_data = np.pad(data, pad_width, mode='constant', constant_values=constant_value)\n",
    "    else:\n",
    "        padded_data = np.pad(data, pad_width, mode=pad_mode)\n",
    "\n",
    "    windows = np.array([\n",
    "        padded_data[:, i:i + window_size]\n",
    "        for i in range(0, data.shape[1], stride)\n",
    "    ])\n",
    "    \n",
    "    return windows[..., np.newaxis]\n",
    "\n",
    "\n",
    "def create_binary_labels(midi_windows, threshold=0):\n",
    "    return (np.max(midi_windows, axis=2) > threshold).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8ebfe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = create_sliding_windows(cqt_train, WINDOW_SIZE, STRIDE)\n",
    "Y_windows = create_sliding_windows(piano_train, WINDOW_SIZE, STRIDE, pad_mode='constant', constant_value=0)\n",
    "Y_output = create_binary_labels(Y_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccb9f7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : (551665, 267, 9, 1)\n",
      "Output shape: (551665, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input shape :\", X_input.shape)   # e.g., (n, 267, 9, 1)\n",
    "print(\"Output shape:\", Y_output.shape)  # e.g., (n, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a16ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 252, 8, 10)        330       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 126, 8, 10)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 116, 6, 20)        6620      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 58, 6, 20)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6960)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1782016   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,821,862\n",
      "Trainable params: 1,821,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # --- Model Training ---\n",
    "# log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(10, (16, 2), activation='relu', padding='valid', input_shape=(N_BINS, WINDOW_SIZE, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 1)),\n",
    "    tf.keras.layers.Conv2D(20, (11, 3), activation='relu', padding='valid'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 1)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    \"\"\"Custom F1 score metric (approximated for use in training logs).\"\"\"\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true * y_pred_bin, 'float32'))\n",
    "    predicted_positives = K.sum(K.cast(y_pred_bin, 'float32'))\n",
    "    possible_positives = K.sum(K.cast(y_true, 'float32'))\n",
    "\n",
    "    precision = tp / (predicted_positives + K.epsilon())\n",
    "    recall = tp / (possible_positives + K.epsilon())\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['binary_accuracy'])\n",
    "\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=[\n",
    "#         'binary_accuracy',   # optional but still there\n",
    "#         f1_metric,           # custom F1\n",
    "#         AUC(name='auc'),     # area under curve (optional)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'binary_accuracy',\n",
    "        f1_metric,\n",
    "        AUC(name='auc'),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2476dd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1724/1724 [==============================] - 240s 139ms/step - loss: 0.3108 - binary_accuracy: 0.8791 - f1_metric: 0.0622 - auc: 0.6758 - val_loss: 0.2041 - val_binary_accuracy: 0.9271 - val_f1_metric: 6.4038e-05 - val_auc: 0.8492\n",
      "Epoch 2/10\n",
      "1724/1724 [==============================] - 238s 138ms/step - loss: 0.1728 - binary_accuracy: 0.9439 - f1_metric: 0.0322 - auc: 0.8423 - val_loss: 0.1858 - val_binary_accuracy: 0.9277 - val_f1_metric: 0.0205 - val_auc: 0.8847\n",
      "Epoch 3/10\n",
      "1724/1724 [==============================] - 228s 132ms/step - loss: 0.1463 - binary_accuracy: 0.9467 - f1_metric: 0.2335 - auc: 0.8980 - val_loss: 0.1532 - val_binary_accuracy: 0.9360 - val_f1_metric: 0.2675 - val_auc: 0.9303\n",
      "Epoch 4/10\n",
      "1724/1724 [==============================] - 229s 133ms/step - loss: 0.1135 - binary_accuracy: 0.9560 - f1_metric: 0.4744 - auc: 0.9440 - val_loss: 0.1402 - val_binary_accuracy: 0.9419 - val_f1_metric: 0.4018 - val_auc: 0.9442\n",
      "Epoch 5/10\n",
      "1724/1724 [==============================] - 222s 129ms/step - loss: 0.0977 - binary_accuracy: 0.9614 - f1_metric: 0.5678 - auc: 0.9601 - val_loss: 0.1354 - val_binary_accuracy: 0.9449 - val_f1_metric: 0.4610 - val_auc: 0.9472\n",
      "Epoch 6/10\n",
      "1724/1724 [==============================] - 263s 153ms/step - loss: 0.0897 - binary_accuracy: 0.9641 - f1_metric: 0.6112 - auc: 0.9671 - val_loss: 0.1352 - val_binary_accuracy: 0.9455 - val_f1_metric: 0.4705 - val_auc: 0.9474\n",
      "Epoch 7/10\n",
      "1724/1724 [==============================] - 449s 261ms/step - loss: 0.0849 - binary_accuracy: 0.9657 - f1_metric: 0.6351 - auc: 0.9709 - val_loss: 0.1364 - val_binary_accuracy: 0.9461 - val_f1_metric: 0.4809 - val_auc: 0.9458\n",
      "Epoch 8/10\n",
      "1724/1724 [==============================] - 439s 255ms/step - loss: 0.0814 - binary_accuracy: 0.9669 - f1_metric: 0.6529 - auc: 0.9735 - val_loss: 0.1375 - val_binary_accuracy: 0.9461 - val_f1_metric: 0.4785 - val_auc: 0.9451\n",
      "Epoch 9/10\n",
      "1724/1724 [==============================] - 438s 254ms/step - loss: 0.0791 - binary_accuracy: 0.9676 - f1_metric: 0.6647 - auc: 0.9752 - val_loss: 0.1398 - val_binary_accuracy: 0.9463 - val_f1_metric: 0.4839 - val_auc: 0.9435\n",
      "Epoch 10/10\n",
      "1724/1724 [==============================] - 433s 251ms/step - loss: 0.0772 - binary_accuracy: 0.9683 - f1_metric: 0.6746 - auc: 0.9764 - val_loss: 0.1410 - val_binary_accuracy: 0.9469 - val_f1_metric: 0.4950 - val_auc: 0.9419\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a timestamped log directory\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 2. Create the TensorBoard callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,          # Set to 1 if you want weight histograms (optional)\n",
    "    write_graph=True,          # Log the model graph (default = True)\n",
    "    update_freq='epoch'        # or 'batch' if you want more frequent updates\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-4, restore_best_weights=True)\n",
    "\n",
    "model.fit(\n",
    "    X_input, Y_output,\n",
    "    epochs=10,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, tensorboard_callback]\n",
    ")\n",
    "\n",
    "model.save(\"master_10_0.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f5b904",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6788242",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"master_Earlystop_auc_100ep.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54bdaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
